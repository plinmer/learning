# 1000亿数据去重系统设计方案
## 问题分析
- 数据量：1000亿条URL，平均每个URL 100字节，总数据量约10TB
- 内存限制：32GB/机器
- 存储限制：1TB硬盘/机器
## 核心思路
- hash一致性算法
- mysql的引擎结构，
- hash算法都有那些，具体点差异是啥？
## 问题拆解
- 如何判断重复？
  - 不同hash算法的对比以及性能如何？[哈希算法对比](https://my.oschina.net/emacs_8814310/blog/17326513)

  |算法|长度|计算资源消耗|安全性能|
  |:---:|:---:|:---:|:---:|
  | MD5 | 128 | 低 |低|
  | SHA-1 | 160 | 中|中|
  | SHA-256 | 256 | 中|中|
  | SHA-3 | 从 224 位到 512 位不等 | 高 |高|

  备注：
     - 计算资源消耗：是指计算哈希值的时间和cpu，内存等要求，长度越长一般要求越高
     - 安全性能：指碰撞和破解
  结论：由于是判断文件是否被处理过，对安全性能要求不高，用MD5算法就行
## 如何设计方案，存储到哪里，对应需要多少资源？
- 方案一：直接内存？
  - 资源消耗：128bit * 100 * 10^9 (1000亿) = 1.6TB，显然如果放到内存需要的存储过大
- 方案二：放到硬盘+内存换页方式查找
  - 资源消耗：大概需要2台机器+
  - 数据如何存储：如何设置分片，会存在倾斜问题？还需要保证快速查找？
    - 方案：按照hash值进行分片，将数据切分成多个小文件存储，文件内保证有序 
      - 如何进行数据分片？
        将计算出来的hash值转换成一个数字，按照hash值的前缀进行划分
      - 文件多大合适？
        - 操作系统对于单文件也有限制，文件的加载可以用内存映射的方法来提高效率
        - 同时文件太大加载速度慢，文件内查找性能不好。
        - 文件太小的话，需要频繁换页，索引效率低下。
        - 这里其实是在内存映射、I/O效率、管理复杂度间取得最佳平衡，业界基本的建议是512MB-2GB之间。这里我们可以取1GB来平衡
      - 单分片文件写满了如何处理？如果数据倾斜验证，需要调整分片的路由
        - 继续写：本身我们只限制了文件大小的条数，没有限制文件大小。而且磁盘也是有冗余的，可以支持继续写。——较好
        - 开链法：有点复杂，需要新开一个文件，然后将文件的地址放到当前文件中。
      - 单文件内的存储格式，为了提高查询性能
        - HashMap的序列化结果？
        - sstable，跳表
    - 进一步优化查询性能：布隆过滤器
      - 布隆过滤器的如何设计？有公式，根据存储元素个数和预期误判率能计算出数组长度和hash函数个数
      - 这里有存储要求，会影响机器的个数
## 代码实现：



